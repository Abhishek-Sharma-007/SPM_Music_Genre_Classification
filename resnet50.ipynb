{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7321b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to json\n",
    "DATA_PATH = \"data_10.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8125a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "\n",
    "def load_data(data_path):\n",
    "    with open(data_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # convert lists to numpy arrays\n",
    "    X = np.array(data[\"mfcc\"])\n",
    "    y = np.array(data[\"labels\"])\n",
    "\n",
    "    print(\"Data succesfully loaded!\")\n",
    "    return  X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88512907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data succesfully loaded!\n"
     ]
    }
   ],
   "source": [
    "# Ensure you define DATA_PATH before calling the function or replace DATA_PATH with the actual path string.\n",
    "X, y = load_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33f3d462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (9986, 130, 13)\n",
      "Shape of y: (9986,)\n",
      "\n",
      "First 5 entries of X:\n",
      "[[[ -32.52093887  101.69689941  -12.49266434 ...    0.91859376\n",
      "      6.12813425    7.43504047]\n",
      "  [   4.63589716   92.01476288  -14.43618774 ...   -3.12150717\n",
      "     10.91743088   12.23798275]\n",
      "  [   5.13217115   93.25161743  -15.10477638 ...   -6.58130455\n",
      "      6.40962601    9.9592886 ]\n",
      "  ...\n",
      "  [ -26.90529823   50.987854      7.74356651 ...    5.58424664\n",
      "     -4.36807489   -1.8676579 ]\n",
      "  [ -44.19861984   67.88891602   -1.54656506 ...   -5.91294098\n",
      "     -1.77724183   -1.53501415]\n",
      "  [ -82.0346756    90.83355713   -0.68910015 ...   -9.12341118\n",
      "     -2.63756299   -6.29706573]]\n",
      "\n",
      " [[-118.24279022  105.50691223    2.0165503  ...   -6.59367657\n",
      "     -5.19090176  -20.08145142]\n",
      "  [ -95.36672211   90.11253357   -5.41918373 ...  -13.54324532\n",
      "     -6.37954044  -13.70101547]\n",
      "  [ -82.44755554   74.75208282   -3.18817306 ...  -18.20688629\n",
      "     -9.79681206   -9.82308388]\n",
      "  ...\n",
      "  [  26.10294342   11.1639576    12.10066891 ...   10.06563568\n",
      "      3.96166253    3.04767323]\n",
      "  [  13.23684502    4.94103336   23.89163589 ...    9.79549599\n",
      "      9.21574211   10.5759964 ]\n",
      "  [  -8.53881645   12.11741257   21.36528015 ...    3.55576849\n",
      "      8.36197853   11.7942028 ]]\n",
      "\n",
      " [[ -36.1824379    22.20649529   29.7243309  ...    3.62932467\n",
      "     -2.59775472    0.70207512]\n",
      "  [ -23.52823257   10.69565392   21.35269165 ...    7.47054005\n",
      "     -1.14198136    5.4138341 ]\n",
      "  [ -33.60636139   14.88614941   18.43836975 ...    6.18921041\n",
      "      2.97949386    8.27299118]\n",
      "  ...\n",
      "  [ -23.18297577   41.73841858    6.8090682  ...    1.75160193\n",
      "     -4.28324604   -6.59068346]\n",
      "  [  -1.7148869    40.0640564     0.75233853 ...    0.18402261\n",
      "      3.02147913  -11.38726616]\n",
      "  [ -16.74528885   52.68541718  -13.08858299 ...   -0.39648795\n",
      "      5.28634071  -15.55710983]]\n",
      "\n",
      " [[ -51.61674881   74.5981369   -27.28383255 ...   -2.5674305\n",
      "      1.55957603  -15.60898209]\n",
      "  [ -40.75899506   69.57054901  -32.47380447 ...   -3.85019922\n",
      "      3.13642168  -13.98675632]\n",
      "  [ -63.37325668   66.75404358  -35.3468399  ...  -13.08133507\n",
      "      1.30067265   -9.17732334]\n",
      "  ...\n",
      "  [  -4.18443108   57.25634766  -45.9934082  ...   -1.19724441\n",
      "      6.39828968   11.66631889]\n",
      "  [  15.28538132   50.19358826  -22.9096489  ...    6.90610695\n",
      "     10.99471855    9.59746361]\n",
      "  [ -14.19865131   65.29829407   -5.92649651 ...    1.48104596\n",
      "      8.59943199   11.35733795]]\n",
      "\n",
      " [[ -31.1616497    88.11553192  -11.39753056 ...   -0.7156018\n",
      "      2.23978519   10.35824585]\n",
      "  [ -22.21574593   83.58016968  -23.63366127 ...    1.87400603\n",
      "      7.77310085   12.92164803]\n",
      "  [ -42.16501236   85.88644409  -30.98405075 ...    1.77524889\n",
      "      8.62813377   13.0595932 ]\n",
      "  ...\n",
      "  [ -74.92823029   90.58781433  -19.46250153 ...    1.71460366\n",
      "      5.32834291    9.37605476]\n",
      "  [ -29.04316139   59.2408371    -0.94711143 ...    0.65452564\n",
      "      1.42864358    5.45595074]\n",
      "  [ -24.54699516   53.26959991    8.09057426 ...    2.07323933\n",
      "     -0.50541055    3.20274544]]]\n",
      "\n",
      "First 5 entries of y:\n",
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "def display_data(X, y, num_samples=5):\n",
    "    print(f\"Shape of X: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape}\\n\")\n",
    "\n",
    "    print(f\"First {num_samples} entries of X:\")\n",
    "    print(X[:num_samples])\n",
    "\n",
    "    print(f\"\\nFirst {num_samples} entries of y:\")\n",
    "    print(y[:num_samples])\n",
    "\n",
    "# Call the display function\n",
    "display_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7530d60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# Check the unique labels to determine the number of classes\n",
    "num_classes = len(np.unique(y))\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a4571d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6990, 130, 13) (6990,)\n"
     ]
    }
   ],
   "source": [
    "# create train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d4303f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: [ 0  1  2  3  5  6  7  8  9 10]\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique labels:\", np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "956d131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_labels(y):\n",
    "    # Create a dictionary to map original labels to new labels\n",
    "    unique_labels = np.unique(y)\n",
    "    label_mapping = {original_label: idx for idx, original_label in enumerate(unique_labels)}\n",
    "\n",
    "    # Use the map to transform y\n",
    "    return np.array([label_mapping[label] for label in y])\n",
    "\n",
    "y = transform_labels(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a085b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: [0 1 2 3 4 5 6 7 8 9]\n",
      "Min label: 0\n",
      "Max label: 9\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique labels:\", np.unique(y_train))\n",
    "print(\"Min label:\", y_train.min())\n",
    "print(\"Max label:\", y_train.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30e1c541",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"Blues\", \"Classical\", \"Country\", \"Disco\", \"Hip-Hop\", \"Jazz\", \"Metal\", \"Pop\", \"Reggae\", \"Rock\"]\n",
    "\n",
    "# Based on the unique labels you provided: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10]\n",
    "# Let's create a direct mapping from these labels to the class names:\n",
    "\n",
    "label_to_class_mapping = {idx: class_name for idx, class_name in enumerate(class_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5aac2244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label to class mapping:\n",
      "Label 0: Blues\n",
      "Label 1: Classical\n",
      "Label 2: Country\n",
      "Label 3: Disco\n",
      "Label 4: Hip-Hop\n",
      "Label 5: Jazz\n",
      "Label 6: Metal\n",
      "Label 7: Pop\n",
      "Label 8: Reggae\n",
      "Label 9: Rock\n"
     ]
    }
   ],
   "source": [
    "print(\"Label to class mapping:\")\n",
    "for label, class_name in label_to_class_mapping.items():\n",
    "    print(f\"Label {label}: {class_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22ef3f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min label: 0\n",
      "Max label: 9\n"
     ]
    }
   ],
   "source": [
    "print(\"Min label:\", y_train.min())\n",
    "print(\"Max label:\", y_train.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55d966a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def preprocess_data(X):\n",
    "    # Resize each image in the dataset and duplicate its channel to have 3 channels\n",
    "    X_processed = np.array([cv2.resize(x, (128, 128)) for x in X])\n",
    "    X_processed = np.stack((X_processed,)*3, axis=-1)\n",
    "    return X_processed\n",
    "\n",
    "X_train_processed = preprocess_data(X_train)\n",
    "X_test_processed = preprocess_data(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c9413e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train contains NaN: False\n",
      "X_test contains NaN: False\n",
      "y_train contains NaN: False\n",
      "y_test contains NaN: False\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train contains NaN:\", np.isnan(X_train_processed).any())\n",
    "print(\"X_test contains NaN:\", np.isnan(X_test_processed).any())\n",
    "print(\"y_train contains NaN:\", np.isnan(y_train).any())\n",
    "print(\"y_test contains NaN:\", np.isnan(y_test).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e79e4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 27 01:52:37 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 546.17                 Driver Version: 546.17       CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 ...  WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   39C    P8              12W / 130W |     16MiB /  6144MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     16592    C+G   ...inaries\\Win64\\EpicGamesLauncher.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de294e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout, LeakyReLU, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Load the pre-trained ResNet50 model without the top layer\n",
    "# Adjust the 'input_shape' to match the shape of your spectrograms\n",
    "input_shape = (128, 128, 3)  # Example shape, adjust based on your data preprocessing\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=Input(shape=input_shape))\n",
    "base_model.trainable = False  # Freeze the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d488ae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model on top\n",
    "model_resnet = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "\n",
    "    # Dense layers from model_9\n",
    "    # ... (add your model_9 layers here)\n",
    "    # You can keep using Conv and MaxPooling layers or skip straight to Flatten\n",
    "    # Depending on your dataset, you may experiment with adding additional Conv layers or start flattening\n",
    "    Flatten(),\n",
    "    # Dense layers from model_9\n",
    "    Dense(256, kernel_regularizer=l2(0.001)),\n",
    "    LeakyReLU(),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(128, kernel_regularizer=l2(0.001)),\n",
    "    LeakyReLU(),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(64, kernel_regularizer=l2(0.001)),\n",
    "    LeakyReLU(),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de2c5e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer, compile model, callbacks\n",
    "optimiser = Adam(learning_rate=0.0005)\n",
    "model_resnet.compile(optimizer=optimiser, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f165a025",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint(\"best_resnet50_model.h5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86ec8a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 2.4595 - accuracy: 0.3990\n",
      "Epoch 1: val_loss improved from inf to 1.95384, saving model to best_resnet50_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tf_gpu_env\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 155s 694ms/step - loss: 2.4595 - accuracy: 0.3990 - val_loss: 1.9538 - val_accuracy: 0.5347 - lr: 5.0000e-04\n",
      "Epoch 2/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 2.0526 - accuracy: 0.4856\n",
      "Epoch 2: val_loss improved from 1.95384 to 1.83336, saving model to best_resnet50_model.h5\n",
      "219/219 [==============================] - 144s 657ms/step - loss: 2.0526 - accuracy: 0.4856 - val_loss: 1.8334 - val_accuracy: 0.5494 - lr: 5.0000e-04\n",
      "Epoch 3/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.8465 - accuracy: 0.5475\n",
      "Epoch 3: val_loss improved from 1.83336 to 1.65746, saving model to best_resnet50_model.h5\n",
      "219/219 [==============================] - 136s 622ms/step - loss: 1.8465 - accuracy: 0.5475 - val_loss: 1.6575 - val_accuracy: 0.6011 - lr: 5.0000e-04\n",
      "Epoch 4/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.7077 - accuracy: 0.5761\n",
      "Epoch 4: val_loss improved from 1.65746 to 1.62877, saving model to best_resnet50_model.h5\n",
      "219/219 [==============================] - 131s 599ms/step - loss: 1.7077 - accuracy: 0.5761 - val_loss: 1.6288 - val_accuracy: 0.5898 - lr: 5.0000e-04\n",
      "Epoch 5/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.5930 - accuracy: 0.6037\n",
      "Epoch 5: val_loss improved from 1.62877 to 1.49169, saving model to best_resnet50_model.h5\n",
      "219/219 [==============================] - 145s 664ms/step - loss: 1.5930 - accuracy: 0.6037 - val_loss: 1.4917 - val_accuracy: 0.6368 - lr: 5.0000e-04\n",
      "Epoch 6/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.5447 - accuracy: 0.6126\n",
      "Epoch 6: val_loss did not improve from 1.49169\n",
      "219/219 [==============================] - 141s 643ms/step - loss: 1.5447 - accuracy: 0.6126 - val_loss: 1.5098 - val_accuracy: 0.6225 - lr: 5.0000e-04\n",
      "Epoch 7/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.4686 - accuracy: 0.6333\n",
      "Epoch 7: val_loss improved from 1.49169 to 1.43590, saving model to best_resnet50_model.h5\n",
      "219/219 [==============================] - 149s 682ms/step - loss: 1.4686 - accuracy: 0.6333 - val_loss: 1.4359 - val_accuracy: 0.6415 - lr: 5.0000e-04\n",
      "Epoch 8/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.4130 - accuracy: 0.6535\n",
      "Epoch 8: val_loss did not improve from 1.43590\n",
      "219/219 [==============================] - 158s 722ms/step - loss: 1.4130 - accuracy: 0.6535 - val_loss: 1.5159 - val_accuracy: 0.6172 - lr: 5.0000e-04\n",
      "Epoch 9/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.3576 - accuracy: 0.6654\n",
      "Epoch 9: val_loss did not improve from 1.43590\n",
      "219/219 [==============================] - 149s 683ms/step - loss: 1.3576 - accuracy: 0.6654 - val_loss: 1.5147 - val_accuracy: 0.6152 - lr: 5.0000e-04\n",
      "Epoch 10/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.3303 - accuracy: 0.6724\n",
      "Epoch 10: val_loss improved from 1.43590 to 1.37816, saving model to best_resnet50_model.h5\n",
      "219/219 [==============================] - 156s 713ms/step - loss: 1.3303 - accuracy: 0.6724 - val_loss: 1.3782 - val_accuracy: 0.6502 - lr: 5.0000e-04\n",
      "Epoch 11/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.2917 - accuracy: 0.6778\n",
      "Epoch 11: val_loss improved from 1.37816 to 1.31131, saving model to best_resnet50_model.h5\n",
      "219/219 [==============================] - 148s 679ms/step - loss: 1.2917 - accuracy: 0.6778 - val_loss: 1.3113 - val_accuracy: 0.6759 - lr: 5.0000e-04\n",
      "Epoch 12/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.2515 - accuracy: 0.6868\n",
      "Epoch 12: val_loss did not improve from 1.31131\n",
      "219/219 [==============================] - 150s 686ms/step - loss: 1.2515 - accuracy: 0.6868 - val_loss: 1.4408 - val_accuracy: 0.6302 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.2355 - accuracy: 0.6954\n",
      "Epoch 13: val_loss did not improve from 1.31131\n",
      "219/219 [==============================] - 139s 637ms/step - loss: 1.2355 - accuracy: 0.6954 - val_loss: 1.3383 - val_accuracy: 0.6512 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.1984 - accuracy: 0.7004\n",
      "Epoch 14: val_loss did not improve from 1.31131\n",
      "219/219 [==============================] - 150s 684ms/step - loss: 1.1984 - accuracy: 0.7004 - val_loss: 1.3772 - val_accuracy: 0.6545 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.1917 - accuracy: 0.7016\n",
      "Epoch 15: val_loss improved from 1.31131 to 1.30645, saving model to best_resnet50_model.h5\n",
      "219/219 [==============================] - 152s 696ms/step - loss: 1.1917 - accuracy: 0.7016 - val_loss: 1.3065 - val_accuracy: 0.6712 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.1825 - accuracy: 0.7087\n",
      "Epoch 16: val_loss did not improve from 1.30645\n",
      "219/219 [==============================] - 148s 677ms/step - loss: 1.1825 - accuracy: 0.7087 - val_loss: 1.3076 - val_accuracy: 0.6756 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.1610 - accuracy: 0.7142\n",
      "Epoch 17: val_loss did not improve from 1.30645\n",
      "219/219 [==============================] - 188s 859ms/step - loss: 1.1610 - accuracy: 0.7142 - val_loss: 1.3193 - val_accuracy: 0.6612 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.1039 - accuracy: 0.7272\n",
      "Epoch 18: val_loss did not improve from 1.30645\n",
      "219/219 [==============================] - 149s 681ms/step - loss: 1.1039 - accuracy: 0.7272 - val_loss: 1.3272 - val_accuracy: 0.6555 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.1336 - accuracy: 0.7202\n",
      "Epoch 19: val_loss did not improve from 1.30645\n",
      "219/219 [==============================] - 149s 680ms/step - loss: 1.1336 - accuracy: 0.7202 - val_loss: 1.4511 - val_accuracy: 0.6292 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.1054 - accuracy: 0.7179\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 20: val_loss did not improve from 1.30645\n",
      "219/219 [==============================] - 145s 663ms/step - loss: 1.1054 - accuracy: 0.7179 - val_loss: 1.3190 - val_accuracy: 0.6529 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.0097 - accuracy: 0.7528\n",
      "Epoch 21: val_loss improved from 1.30645 to 1.19633, saving model to best_resnet50_model.h5\n",
      "219/219 [==============================] - 149s 683ms/step - loss: 1.0097 - accuracy: 0.7528 - val_loss: 1.1963 - val_accuracy: 0.6939 - lr: 2.5000e-04\n",
      "Epoch 22/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9781 - accuracy: 0.7681\n",
      "Epoch 22: val_loss did not improve from 1.19633\n",
      "219/219 [==============================] - 138s 629ms/step - loss: 0.9781 - accuracy: 0.7681 - val_loss: 1.2104 - val_accuracy: 0.6872 - lr: 2.5000e-04\n",
      "Epoch 23/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9466 - accuracy: 0.7731\n",
      "Epoch 23: val_loss did not improve from 1.19633\n",
      "219/219 [==============================] - 136s 624ms/step - loss: 0.9466 - accuracy: 0.7731 - val_loss: 1.2166 - val_accuracy: 0.6816 - lr: 2.5000e-04\n",
      "Epoch 24/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9222 - accuracy: 0.7798\n",
      "Epoch 24: val_loss did not improve from 1.19633\n",
      "219/219 [==============================] - 139s 636ms/step - loss: 0.9222 - accuracy: 0.7798 - val_loss: 1.2411 - val_accuracy: 0.6819 - lr: 2.5000e-04\n",
      "Epoch 25/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9166 - accuracy: 0.7827\n",
      "Epoch 25: val_loss did not improve from 1.19633\n",
      "219/219 [==============================] - 139s 636ms/step - loss: 0.9166 - accuracy: 0.7827 - val_loss: 1.2252 - val_accuracy: 0.6862 - lr: 2.5000e-04\n",
      "Epoch 26/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9308 - accuracy: 0.7707\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 26: val_loss did not improve from 1.19633\n",
      "219/219 [==============================] - 143s 656ms/step - loss: 0.9308 - accuracy: 0.7707 - val_loss: 1.2129 - val_accuracy: 0.6946 - lr: 2.5000e-04\n",
      "Epoch 27/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.8461 - accuracy: 0.8072\n",
      "Epoch 27: val_loss improved from 1.19633 to 1.15615, saving model to best_resnet50_model.h5\n",
      "219/219 [==============================] - 150s 684ms/step - loss: 0.8461 - accuracy: 0.8072 - val_loss: 1.1561 - val_accuracy: 0.6929 - lr: 1.2500e-04\n",
      "Epoch 28/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.8434 - accuracy: 0.8034\n",
      "Epoch 28: val_loss improved from 1.15615 to 1.15503, saving model to best_resnet50_model.h5\n",
      "219/219 [==============================] - 141s 646ms/step - loss: 0.8434 - accuracy: 0.8034 - val_loss: 1.1550 - val_accuracy: 0.6999 - lr: 1.2500e-04\n",
      "Epoch 29/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.8120 - accuracy: 0.8119\n",
      "Epoch 29: val_loss improved from 1.15503 to 1.14851, saving model to best_resnet50_model.h5\n",
      "219/219 [==============================] - 144s 661ms/step - loss: 0.8120 - accuracy: 0.8119 - val_loss: 1.1485 - val_accuracy: 0.7086 - lr: 1.2500e-04\n",
      "Epoch 30/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.8019 - accuracy: 0.8116\n",
      "Epoch 30: val_loss did not improve from 1.14851\n",
      "219/219 [==============================] - 146s 668ms/step - loss: 0.8019 - accuracy: 0.8116 - val_loss: 1.1569 - val_accuracy: 0.7106 - lr: 1.2500e-04\n",
      "Epoch 31/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.7962 - accuracy: 0.8180\n",
      "Epoch 31: val_loss did not improve from 1.14851\n",
      "219/219 [==============================] - 147s 674ms/step - loss: 0.7962 - accuracy: 0.8180 - val_loss: 1.1568 - val_accuracy: 0.7036 - lr: 1.2500e-04\n",
      "Epoch 32/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.7855 - accuracy: 0.8163\n",
      "Epoch 32: val_loss did not improve from 1.14851\n",
      "219/219 [==============================] - 132s 605ms/step - loss: 0.7855 - accuracy: 0.8163 - val_loss: 1.1691 - val_accuracy: 0.6966 - lr: 1.2500e-04\n",
      "Epoch 33/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.7800 - accuracy: 0.8113\n",
      "Epoch 33: val_loss did not improve from 1.14851\n",
      "219/219 [==============================] - 141s 645ms/step - loss: 0.7800 - accuracy: 0.8113 - val_loss: 1.1834 - val_accuracy: 0.7013 - lr: 1.2500e-04\n",
      "Epoch 34/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.7603 - accuracy: 0.8259\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 34: val_loss did not improve from 1.14851\n",
      "219/219 [==============================] - 145s 662ms/step - loss: 0.7603 - accuracy: 0.8259 - val_loss: 1.2241 - val_accuracy: 0.6916 - lr: 1.2500e-04\n",
      "Epoch 35/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.7406 - accuracy: 0.8338\n",
      "Epoch 35: val_loss improved from 1.14851 to 1.14151, saving model to best_resnet50_model.h5\n",
      "219/219 [==============================] - 150s 687ms/step - loss: 0.7406 - accuracy: 0.8338 - val_loss: 1.1415 - val_accuracy: 0.7126 - lr: 6.2500e-05\n",
      "Epoch 36/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.7167 - accuracy: 0.8455\n",
      "Epoch 36: val_loss did not improve from 1.14151\n",
      "219/219 [==============================] - 143s 656ms/step - loss: 0.7167 - accuracy: 0.8455 - val_loss: 1.1529 - val_accuracy: 0.7049 - lr: 6.2500e-05\n",
      "Epoch 37/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.7166 - accuracy: 0.8366\n",
      "Epoch 37: val_loss did not improve from 1.14151\n",
      "219/219 [==============================] - 144s 658ms/step - loss: 0.7166 - accuracy: 0.8366 - val_loss: 1.1423 - val_accuracy: 0.7073 - lr: 6.2500e-05\n",
      "Epoch 38/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.7145 - accuracy: 0.8372\n",
      "Epoch 38: val_loss did not improve from 1.14151\n",
      "219/219 [==============================] - 136s 622ms/step - loss: 0.7145 - accuracy: 0.8372 - val_loss: 1.1552 - val_accuracy: 0.7119 - lr: 6.2500e-05\n",
      "Epoch 39/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.7023 - accuracy: 0.8423\n",
      "Epoch 39: val_loss did not improve from 1.14151\n",
      "219/219 [==============================] - 140s 642ms/step - loss: 0.7023 - accuracy: 0.8423 - val_loss: 1.1515 - val_accuracy: 0.7036 - lr: 6.2500e-05\n",
      "Epoch 40/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.6894 - accuracy: 0.8505\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 40: val_loss did not improve from 1.14151\n",
      "219/219 [==============================] - 145s 665ms/step - loss: 0.6894 - accuracy: 0.8505 - val_loss: 1.1679 - val_accuracy: 0.7019 - lr: 6.2500e-05\n",
      "Epoch 41/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.6677 - accuracy: 0.8581\n",
      "Epoch 41: val_loss did not improve from 1.14151\n",
      "219/219 [==============================] - 145s 665ms/step - loss: 0.6677 - accuracy: 0.8581 - val_loss: 1.1440 - val_accuracy: 0.7153 - lr: 3.1250e-05\n",
      "Epoch 42/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.6767 - accuracy: 0.8545\n",
      "Epoch 42: val_loss did not improve from 1.14151\n",
      "219/219 [==============================] - 129s 590ms/step - loss: 0.6767 - accuracy: 0.8545 - val_loss: 1.1440 - val_accuracy: 0.7133 - lr: 3.1250e-05\n",
      "Epoch 43/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.6618 - accuracy: 0.8585\n",
      "Epoch 43: val_loss did not improve from 1.14151\n",
      "219/219 [==============================] - 127s 582ms/step - loss: 0.6618 - accuracy: 0.8585 - val_loss: 1.1496 - val_accuracy: 0.7093 - lr: 3.1250e-05\n",
      "Epoch 44/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.6532 - accuracy: 0.8595\n",
      "Epoch 44: val_loss did not improve from 1.14151\n",
      "219/219 [==============================] - 128s 584ms/step - loss: 0.6532 - accuracy: 0.8595 - val_loss: 1.1525 - val_accuracy: 0.7076 - lr: 3.1250e-05\n",
      "Epoch 45/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.6549 - accuracy: 0.8577\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 45: val_loss did not improve from 1.14151\n",
      "219/219 [==============================] - 130s 597ms/step - loss: 0.6549 - accuracy: 0.8577 - val_loss: 1.1515 - val_accuracy: 0.7099 - lr: 3.1250e-05\n"
     ]
    }
   ],
   "source": [
    "# Now you can fit the model with the processed data\n",
    "history = model_resnet.fit(X_train_processed, y_train, \n",
    "                             validation_data=(X_test_processed, y_test), \n",
    "                             batch_size=32, epochs=100, \n",
    "                             callbacks=[lr_scheduler, early_stopping, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19d4ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab4bb16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b507453f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c64934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb4aa29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80da89b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f6d35b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e089ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4c7ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2f3505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b38d50f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e33209b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e602f8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import DenseNet201\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, BatchNormalization, Dropout, LeakyReLU, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Load the pre-trained DenseNet201 model without the top layer\n",
    "input_shape = (224, 224, 3)  # Example shape, adjust based on your data preprocessing\n",
    "base_model = DenseNet201(weights='imagenet', include_top=False, input_tensor=Input(shape=input_shape))\n",
    "base_model.trainable = False  # Freeze the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2d78022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model on top\n",
    "model_densenet = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "\n",
    "    # Dense layers\n",
    "    Dense(256, kernel_regularizer=l2(0.001)),\n",
    "    LeakyReLU(),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(128, kernel_regularizer=l2(0.001)),\n",
    "    LeakyReLU(),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(64, kernel_regularizer=l2(0.001)),\n",
    "    LeakyReLU(),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Output layer\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3dbfcc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer, compile model, callbacks\n",
    "optimiser = Adam(learning_rate=0.0005)\n",
    "model_densenet.compile(optimizer=optimiser, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "721171c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint(\"best_densenet201_model.h5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3dbe110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 56/219 [======>.......................] - ETA: 5:34 - loss: 2.8411 - accuracy: 0.2796"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Now you can fit the model with the processed data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_densenet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_processed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test_processed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\tf_gpu_env\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\tf_gpu_env\\lib\\site-packages\\keras\\src\\engine\\training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1776\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1777\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1780\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1781\u001b[0m ):\n\u001b[0;32m   1782\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1783\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1785\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\tf_gpu_env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\tf_gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    828\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 831\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    833\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    834\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\tf_gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    864\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\tf_gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\tf_gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1260\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1262\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1263\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1265\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1266\u001b[0m     args,\n\u001b[0;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1268\u001b[0m     executing_eagerly)\n\u001b[0;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\tf_gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    216\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\tf_gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    251\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 252\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    262\u001b[0m     )\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\tf_gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1477\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1479\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1480\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1481\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1482\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1483\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1484\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1485\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1487\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1488\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1489\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1493\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1494\u001b[0m   )\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\tf_gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[0;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[0;32m     59\u001b[0m   ]\n\u001b[1;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Now you can fit the model with the processed data\n",
    "history = model_densenet.fit(X_train_processed, y_train, \n",
    "                             validation_data=(X_test_processed, y_test), \n",
    "                             batch_size=32, epochs=100, \n",
    "                             callbacks=[lr_scheduler, early_stopping, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ecf75b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5613221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b40257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a6dbb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87bcc8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout, LeakyReLU, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Load the pre-trained DenseNet121 model without the top layer\n",
    "# Adjust the 'input_shape' to match the shape of your spectrograms\n",
    "input_shape = (128, 128, 3)  # Example shape, adjust based on your data preprocessing\n",
    "base_model = DenseNet121(weights='imagenet', include_top=False, input_tensor=Input(shape=input_shape))\n",
    "base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "# Create a new model on top\n",
    "model_densenet = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "\n",
    "    # Layers from model_9\n",
    "    # Add your model_9 layers here\n",
    "    # You can keep using Conv and MaxPooling layers or skip straight to Flatten\n",
    "    Flatten(),\n",
    "    Dense(256, kernel_regularizer=l2(0.001)),\n",
    "    LeakyReLU(),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(128, kernel_regularizer=l2(0.001)),\n",
    "    LeakyReLU(),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(64, kernel_regularizer=l2(0.001)),\n",
    "    LeakyReLU(),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Output layer\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3673a2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer, compile model, and define callbacks\n",
    "optimiser = Adam(learning_rate=0.0005)\n",
    "model_densenet.compile(optimizer=optimiser, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint(\"best_densenet_model.h5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d83ad95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 2.4063 - accuracy: 0.3619\n",
      "Epoch 1: val_loss improved from inf to 1.98203, saving model to best_densenet_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tf_gpu_env\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 175s 780ms/step - loss: 2.4063 - accuracy: 0.3619 - val_loss: 1.9820 - val_accuracy: 0.4499 - lr: 5.0000e-04\n",
      "Epoch 2/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 2.0141 - accuracy: 0.4475\n",
      "Epoch 2: val_loss improved from 1.98203 to 1.84227, saving model to best_densenet_model.h5\n",
      "219/219 [==============================] - 225s 1s/step - loss: 2.0141 - accuracy: 0.4475 - val_loss: 1.8423 - val_accuracy: 0.4710 - lr: 5.0000e-04\n",
      "Epoch 3/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.8960 - accuracy: 0.4805\n",
      "Epoch 3: val_loss improved from 1.84227 to 1.71029, saving model to best_densenet_model.h5\n",
      "219/219 [==============================] - 213s 973ms/step - loss: 1.8960 - accuracy: 0.4805 - val_loss: 1.7103 - val_accuracy: 0.5150 - lr: 5.0000e-04\n",
      "Epoch 4/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.8242 - accuracy: 0.4983\n",
      "Epoch 4: val_loss improved from 1.71029 to 1.61585, saving model to best_densenet_model.h5\n",
      "219/219 [==============================] - 229s 1s/step - loss: 1.8242 - accuracy: 0.4983 - val_loss: 1.6159 - val_accuracy: 0.5474 - lr: 5.0000e-04\n",
      "Epoch 5/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.7394 - accuracy: 0.5189\n",
      "Epoch 5: val_loss improved from 1.61585 to 1.59370, saving model to best_densenet_model.h5\n",
      "219/219 [==============================] - 215s 982ms/step - loss: 1.7394 - accuracy: 0.5189 - val_loss: 1.5937 - val_accuracy: 0.5524 - lr: 5.0000e-04\n",
      "Epoch 6/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.7089 - accuracy: 0.5205\n",
      "Epoch 6: val_loss improved from 1.59370 to 1.59087, saving model to best_densenet_model.h5\n",
      "219/219 [==============================] - 222s 1s/step - loss: 1.7089 - accuracy: 0.5205 - val_loss: 1.5909 - val_accuracy: 0.5691 - lr: 5.0000e-04\n",
      "Epoch 7/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.6388 - accuracy: 0.5402\n",
      "Epoch 7: val_loss improved from 1.59087 to 1.52783, saving model to best_densenet_model.h5\n",
      "219/219 [==============================] - 189s 866ms/step - loss: 1.6388 - accuracy: 0.5402 - val_loss: 1.5278 - val_accuracy: 0.5628 - lr: 5.0000e-04\n",
      "Epoch 8/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.6127 - accuracy: 0.5531\n",
      "Epoch 8: val_loss did not improve from 1.52783\n",
      "219/219 [==============================] - 220s 1s/step - loss: 1.6127 - accuracy: 0.5531 - val_loss: 1.6664 - val_accuracy: 0.5324 - lr: 5.0000e-04\n",
      "Epoch 9/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.5723 - accuracy: 0.5575\n",
      "Epoch 9: val_loss did not improve from 1.52783\n",
      "219/219 [==============================] - 204s 933ms/step - loss: 1.5723 - accuracy: 0.5575 - val_loss: 1.5534 - val_accuracy: 0.5561 - lr: 5.0000e-04\n",
      "Epoch 10/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.5244 - accuracy: 0.5761\n",
      "Epoch 10: val_loss improved from 1.52783 to 1.51221, saving model to best_densenet_model.h5\n",
      "219/219 [==============================] - 206s 943ms/step - loss: 1.5244 - accuracy: 0.5761 - val_loss: 1.5122 - val_accuracy: 0.5738 - lr: 5.0000e-04\n",
      "Epoch 11/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.4973 - accuracy: 0.5821\n",
      "Epoch 11: val_loss improved from 1.51221 to 1.43066, saving model to best_densenet_model.h5\n",
      "219/219 [==============================] - 203s 926ms/step - loss: 1.4973 - accuracy: 0.5821 - val_loss: 1.4307 - val_accuracy: 0.5898 - lr: 5.0000e-04\n",
      "Epoch 12/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.4804 - accuracy: 0.5771\n",
      "Epoch 12: val_loss did not improve from 1.43066\n",
      "219/219 [==============================] - 196s 894ms/step - loss: 1.4804 - accuracy: 0.5771 - val_loss: 1.5036 - val_accuracy: 0.5394 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.4566 - accuracy: 0.5951\n",
      "Epoch 13: val_loss did not improve from 1.43066\n",
      "219/219 [==============================] - 198s 907ms/step - loss: 1.4566 - accuracy: 0.5951 - val_loss: 1.6631 - val_accuracy: 0.4927 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.4407 - accuracy: 0.5951\n",
      "Epoch 14: val_loss did not improve from 1.43066\n",
      "219/219 [==============================] - 157s 719ms/step - loss: 1.4407 - accuracy: 0.5951 - val_loss: 1.5168 - val_accuracy: 0.5638 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.4184 - accuracy: 0.5967\n",
      "Epoch 15: val_loss improved from 1.43066 to 1.33429, saving model to best_densenet_model.h5\n",
      "219/219 [==============================] - 162s 741ms/step - loss: 1.4184 - accuracy: 0.5967 - val_loss: 1.3343 - val_accuracy: 0.6182 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.3878 - accuracy: 0.6044\n",
      "Epoch 16: val_loss did not improve from 1.33429\n",
      "219/219 [==============================] - 170s 777ms/step - loss: 1.3878 - accuracy: 0.6044 - val_loss: 1.4143 - val_accuracy: 0.5774 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.3879 - accuracy: 0.6007\n",
      "Epoch 17: val_loss did not improve from 1.33429\n",
      "219/219 [==============================] - 163s 746ms/step - loss: 1.3879 - accuracy: 0.6007 - val_loss: 1.4934 - val_accuracy: 0.5511 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.3632 - accuracy: 0.6150\n",
      "Epoch 18: val_loss did not improve from 1.33429\n",
      "219/219 [==============================] - 165s 753ms/step - loss: 1.3632 - accuracy: 0.6150 - val_loss: 1.4250 - val_accuracy: 0.5841 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.3688 - accuracy: 0.6060\n",
      "Epoch 19: val_loss did not improve from 1.33429\n",
      "219/219 [==============================] - 164s 749ms/step - loss: 1.3688 - accuracy: 0.6060 - val_loss: 1.5390 - val_accuracy: 0.4987 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.3330 - accuracy: 0.6137\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 20: val_loss did not improve from 1.33429\n",
      "219/219 [==============================] - 160s 731ms/step - loss: 1.3330 - accuracy: 0.6137 - val_loss: 1.4170 - val_accuracy: 0.5801 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.2685 - accuracy: 0.6346\n",
      "Epoch 21: val_loss improved from 1.33429 to 1.30333, saving model to best_densenet_model.h5\n",
      "219/219 [==============================] - 180s 822ms/step - loss: 1.2685 - accuracy: 0.6346 - val_loss: 1.3033 - val_accuracy: 0.6078 - lr: 2.5000e-04\n",
      "Epoch 22/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.2462 - accuracy: 0.6534\n",
      "Epoch 22: val_loss did not improve from 1.30333\n",
      "219/219 [==============================] - 154s 704ms/step - loss: 1.2462 - accuracy: 0.6534 - val_loss: 1.3054 - val_accuracy: 0.6091 - lr: 2.5000e-04\n",
      "Epoch 23/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.2414 - accuracy: 0.6432\n",
      "Epoch 23: val_loss improved from 1.30333 to 1.23953, saving model to best_densenet_model.h5\n",
      "219/219 [==============================] - 209s 958ms/step - loss: 1.2414 - accuracy: 0.6432 - val_loss: 1.2395 - val_accuracy: 0.6308 - lr: 2.5000e-04\n",
      "Epoch 24/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.2054 - accuracy: 0.6567\n",
      "Epoch 24: val_loss did not improve from 1.23953\n",
      "219/219 [==============================] - 225s 1s/step - loss: 1.2054 - accuracy: 0.6567 - val_loss: 1.2779 - val_accuracy: 0.6212 - lr: 2.5000e-04\n",
      "Epoch 25/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.2321 - accuracy: 0.6458\n",
      "Epoch 25: val_loss did not improve from 1.23953\n",
      "219/219 [==============================] - 171s 781ms/step - loss: 1.2321 - accuracy: 0.6458 - val_loss: 1.2410 - val_accuracy: 0.6358 - lr: 2.5000e-04\n",
      "Epoch 26/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.2003 - accuracy: 0.6572\n",
      "Epoch 26: val_loss did not improve from 1.23953\n",
      "219/219 [==============================] - 176s 804ms/step - loss: 1.2003 - accuracy: 0.6572 - val_loss: 1.2915 - val_accuracy: 0.6168 - lr: 2.5000e-04\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - ETA: 0s - loss: 1.2010 - accuracy: 0.6521\n",
      "Epoch 27: val_loss improved from 1.23953 to 1.16422, saving model to best_densenet_model.h5\n",
      "219/219 [==============================] - 161s 735ms/step - loss: 1.2010 - accuracy: 0.6521 - val_loss: 1.1642 - val_accuracy: 0.6609 - lr: 2.5000e-04\n",
      "Epoch 28/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.1870 - accuracy: 0.6515\n",
      "Epoch 28: val_loss did not improve from 1.16422\n",
      "219/219 [==============================] - 189s 863ms/step - loss: 1.1870 - accuracy: 0.6515 - val_loss: 1.1901 - val_accuracy: 0.6539 - lr: 2.5000e-04\n",
      "Epoch 29/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.1746 - accuracy: 0.6639\n",
      "Epoch 29: val_loss did not improve from 1.16422\n",
      "219/219 [==============================] - 196s 898ms/step - loss: 1.1746 - accuracy: 0.6639 - val_loss: 1.1800 - val_accuracy: 0.6502 - lr: 2.5000e-04\n",
      "Epoch 30/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.1617 - accuracy: 0.6647\n",
      "Epoch 30: val_loss did not improve from 1.16422\n",
      "219/219 [==============================] - 185s 845ms/step - loss: 1.1617 - accuracy: 0.6647 - val_loss: 1.1944 - val_accuracy: 0.6529 - lr: 2.5000e-04\n",
      "Epoch 31/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.1626 - accuracy: 0.6682\n",
      "Epoch 31: val_loss did not improve from 1.16422\n",
      "219/219 [==============================] - 171s 784ms/step - loss: 1.1626 - accuracy: 0.6682 - val_loss: 1.1837 - val_accuracy: 0.6502 - lr: 2.5000e-04\n",
      "Epoch 32/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.1571 - accuracy: 0.6690\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 32: val_loss did not improve from 1.16422\n",
      "219/219 [==============================] - 166s 759ms/step - loss: 1.1571 - accuracy: 0.6690 - val_loss: 1.2136 - val_accuracy: 0.6382 - lr: 2.5000e-04\n",
      "Epoch 33/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.1314 - accuracy: 0.6722\n",
      "Epoch 33: val_loss improved from 1.16422 to 1.09024, saving model to best_densenet_model.h5\n",
      "219/219 [==============================] - 180s 822ms/step - loss: 1.1314 - accuracy: 0.6722 - val_loss: 1.0902 - val_accuracy: 0.6842 - lr: 1.2500e-04\n",
      "Epoch 34/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.1080 - accuracy: 0.6825\n",
      "Epoch 34: val_loss did not improve from 1.09024\n",
      "219/219 [==============================] - 182s 831ms/step - loss: 1.1080 - accuracy: 0.6825 - val_loss: 1.1172 - val_accuracy: 0.6696 - lr: 1.2500e-04\n",
      "Epoch 35/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.0884 - accuracy: 0.6903\n",
      "Epoch 35: val_loss improved from 1.09024 to 1.07978, saving model to best_densenet_model.h5\n",
      "219/219 [==============================] - 183s 835ms/step - loss: 1.0884 - accuracy: 0.6903 - val_loss: 1.0798 - val_accuracy: 0.6889 - lr: 1.2500e-04\n",
      "Epoch 36/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.0750 - accuracy: 0.6933\n",
      "Epoch 36: val_loss did not improve from 1.07978\n",
      "219/219 [==============================] - 168s 767ms/step - loss: 1.0750 - accuracy: 0.6933 - val_loss: 1.1125 - val_accuracy: 0.6769 - lr: 1.2500e-04\n",
      "Epoch 37/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.0883 - accuracy: 0.6848\n",
      "Epoch 37: val_loss did not improve from 1.07978\n",
      "219/219 [==============================] - 161s 736ms/step - loss: 1.0883 - accuracy: 0.6848 - val_loss: 1.1118 - val_accuracy: 0.6736 - lr: 1.2500e-04\n",
      "Epoch 38/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.0687 - accuracy: 0.6893\n",
      "Epoch 38: val_loss improved from 1.07978 to 1.07297, saving model to best_densenet_model.h5\n",
      "219/219 [==============================] - 171s 784ms/step - loss: 1.0687 - accuracy: 0.6893 - val_loss: 1.0730 - val_accuracy: 0.6809 - lr: 1.2500e-04\n",
      "Epoch 39/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.0701 - accuracy: 0.6897\n",
      "Epoch 39: val_loss improved from 1.07297 to 1.06241, saving model to best_densenet_model.h5\n",
      "219/219 [==============================] - 168s 767ms/step - loss: 1.0701 - accuracy: 0.6897 - val_loss: 1.0624 - val_accuracy: 0.6862 - lr: 1.2500e-04\n",
      "Epoch 40/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.0636 - accuracy: 0.6933\n",
      "Epoch 40: val_loss improved from 1.06241 to 1.04507, saving model to best_densenet_model.h5\n",
      "219/219 [==============================] - 168s 769ms/step - loss: 1.0636 - accuracy: 0.6933 - val_loss: 1.0451 - val_accuracy: 0.6993 - lr: 1.2500e-04\n",
      "Epoch 41/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.0674 - accuracy: 0.6828\n",
      "Epoch 41: val_loss did not improve from 1.04507\n",
      "219/219 [==============================] - 166s 760ms/step - loss: 1.0674 - accuracy: 0.6828 - val_loss: 1.1163 - val_accuracy: 0.6662 - lr: 1.2500e-04\n",
      "Epoch 42/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.0546 - accuracy: 0.6963\n",
      "Epoch 42: val_loss did not improve from 1.04507\n",
      "219/219 [==============================] - 157s 719ms/step - loss: 1.0546 - accuracy: 0.6963 - val_loss: 1.0750 - val_accuracy: 0.6836 - lr: 1.2500e-04\n",
      "Epoch 43/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.0587 - accuracy: 0.6990\n",
      "Epoch 43: val_loss did not improve from 1.04507\n",
      "219/219 [==============================] - 155s 709ms/step - loss: 1.0587 - accuracy: 0.6990 - val_loss: 1.0638 - val_accuracy: 0.6846 - lr: 1.2500e-04\n",
      "Epoch 44/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.0473 - accuracy: 0.6950\n",
      "Epoch 44: val_loss did not improve from 1.04507\n",
      "219/219 [==============================] - 174s 796ms/step - loss: 1.0473 - accuracy: 0.6950 - val_loss: 1.0632 - val_accuracy: 0.6943 - lr: 1.2500e-04\n",
      "Epoch 45/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.0419 - accuracy: 0.6984\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 45: val_loss did not improve from 1.04507\n",
      "219/219 [==============================] - 174s 795ms/step - loss: 1.0419 - accuracy: 0.6984 - val_loss: 1.1039 - val_accuracy: 0.6706 - lr: 1.2500e-04\n",
      "Epoch 46/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.0122 - accuracy: 0.7060\n",
      "Epoch 46: val_loss did not improve from 1.04507\n",
      "219/219 [==============================] - 147s 671ms/step - loss: 1.0122 - accuracy: 0.7060 - val_loss: 1.0948 - val_accuracy: 0.6836 - lr: 6.2500e-05\n",
      "Epoch 47/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.0039 - accuracy: 0.7150\n",
      "Epoch 47: val_loss improved from 1.04507 to 1.03780, saving model to best_densenet_model.h5\n",
      "219/219 [==============================] - 151s 692ms/step - loss: 1.0039 - accuracy: 0.7150 - val_loss: 1.0378 - val_accuracy: 0.6886 - lr: 6.2500e-05\n",
      "Epoch 48/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9944 - accuracy: 0.7139\n",
      "Epoch 48: val_loss improved from 1.03780 to 1.03048, saving model to best_densenet_model.h5\n",
      "219/219 [==============================] - 159s 726ms/step - loss: 0.9944 - accuracy: 0.7139 - val_loss: 1.0305 - val_accuracy: 0.6956 - lr: 6.2500e-05\n",
      "Epoch 49/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.0016 - accuracy: 0.7092\n",
      "Epoch 49: val_loss did not improve from 1.03048\n",
      "219/219 [==============================] - 146s 668ms/step - loss: 1.0016 - accuracy: 0.7092 - val_loss: 1.0574 - val_accuracy: 0.6852 - lr: 6.2500e-05\n",
      "Epoch 50/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9788 - accuracy: 0.7229\n",
      "Epoch 50: val_loss improved from 1.03048 to 1.02489, saving model to best_densenet_model.h5\n",
      "219/219 [==============================] - 150s 686ms/step - loss: 0.9788 - accuracy: 0.7229 - val_loss: 1.0249 - val_accuracy: 0.6943 - lr: 6.2500e-05\n",
      "Epoch 51/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9959 - accuracy: 0.7157\n",
      "Epoch 51: val_loss did not improve from 1.02489\n",
      "219/219 [==============================] - 150s 685ms/step - loss: 0.9959 - accuracy: 0.7157 - val_loss: 1.0324 - val_accuracy: 0.7089 - lr: 6.2500e-05\n",
      "Epoch 52/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9887 - accuracy: 0.7089\n",
      "Epoch 52: val_loss improved from 1.02489 to 1.01722, saving model to best_densenet_model.h5\n",
      "219/219 [==============================] - 147s 674ms/step - loss: 0.9887 - accuracy: 0.7089 - val_loss: 1.0172 - val_accuracy: 0.6999 - lr: 6.2500e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9640 - accuracy: 0.7293\n",
      "Epoch 53: val_loss improved from 1.01722 to 1.00853, saving model to best_densenet_model.h5\n",
      "219/219 [==============================] - 140s 640ms/step - loss: 0.9640 - accuracy: 0.7293 - val_loss: 1.0085 - val_accuracy: 0.7126 - lr: 6.2500e-05\n",
      "Epoch 54/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9705 - accuracy: 0.7236\n",
      "Epoch 54: val_loss did not improve from 1.00853\n",
      "219/219 [==============================] - 144s 656ms/step - loss: 0.9705 - accuracy: 0.7236 - val_loss: 1.0234 - val_accuracy: 0.6939 - lr: 6.2500e-05\n",
      "Epoch 55/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9777 - accuracy: 0.7189\n",
      "Epoch 55: val_loss did not improve from 1.00853\n",
      "219/219 [==============================] - 148s 675ms/step - loss: 0.9777 - accuracy: 0.7189 - val_loss: 1.0166 - val_accuracy: 0.6996 - lr: 6.2500e-05\n",
      "Epoch 56/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9736 - accuracy: 0.7223\n",
      "Epoch 56: val_loss did not improve from 1.00853\n",
      "219/219 [==============================] - 148s 679ms/step - loss: 0.9736 - accuracy: 0.7223 - val_loss: 1.0328 - val_accuracy: 0.6906 - lr: 6.2500e-05\n",
      "Epoch 57/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9745 - accuracy: 0.7150\n",
      "Epoch 57: val_loss did not improve from 1.00853\n",
      "219/219 [==============================] - 149s 682ms/step - loss: 0.9745 - accuracy: 0.7150 - val_loss: 1.0248 - val_accuracy: 0.7003 - lr: 6.2500e-05\n",
      "Epoch 58/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9683 - accuracy: 0.7203\n",
      "Epoch 58: val_loss improved from 1.00853 to 1.00108, saving model to best_densenet_model.h5\n",
      "219/219 [==============================] - 149s 682ms/step - loss: 0.9683 - accuracy: 0.7203 - val_loss: 1.0011 - val_accuracy: 0.7009 - lr: 6.2500e-05\n",
      "Epoch 59/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9659 - accuracy: 0.7283\n",
      "Epoch 59: val_loss did not improve from 1.00108\n",
      "219/219 [==============================] - 160s 732ms/step - loss: 0.9659 - accuracy: 0.7283 - val_loss: 1.0154 - val_accuracy: 0.7009 - lr: 6.2500e-05\n",
      "Epoch 60/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9715 - accuracy: 0.7220\n",
      "Epoch 60: val_loss did not improve from 1.00108\n",
      "219/219 [==============================] - 172s 788ms/step - loss: 0.9715 - accuracy: 0.7220 - val_loss: 1.0189 - val_accuracy: 0.6976 - lr: 6.2500e-05\n",
      "Epoch 61/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9661 - accuracy: 0.7210\n",
      "Epoch 61: val_loss did not improve from 1.00108\n",
      "219/219 [==============================] - 162s 742ms/step - loss: 0.9661 - accuracy: 0.7210 - val_loss: 1.0483 - val_accuracy: 0.6879 - lr: 6.2500e-05\n",
      "Epoch 62/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9535 - accuracy: 0.7272\n",
      "Epoch 62: val_loss did not improve from 1.00108\n",
      "219/219 [==============================] - 172s 788ms/step - loss: 0.9535 - accuracy: 0.7272 - val_loss: 1.0061 - val_accuracy: 0.7019 - lr: 6.2500e-05\n",
      "Epoch 63/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9511 - accuracy: 0.7283\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 63: val_loss did not improve from 1.00108\n",
      "219/219 [==============================] - 168s 766ms/step - loss: 0.9511 - accuracy: 0.7283 - val_loss: 1.0108 - val_accuracy: 0.7036 - lr: 6.2500e-05\n",
      "Epoch 64/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9417 - accuracy: 0.7359\n",
      "Epoch 64: val_loss improved from 1.00108 to 0.99371, saving model to best_densenet_model.h5\n",
      "219/219 [==============================] - 170s 776ms/step - loss: 0.9417 - accuracy: 0.7359 - val_loss: 0.9937 - val_accuracy: 0.7026 - lr: 3.1250e-05\n",
      "Epoch 65/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9267 - accuracy: 0.7373\n",
      "Epoch 65: val_loss improved from 0.99371 to 0.98174, saving model to best_densenet_model.h5\n",
      "219/219 [==============================] - 162s 739ms/step - loss: 0.9267 - accuracy: 0.7373 - val_loss: 0.9817 - val_accuracy: 0.7126 - lr: 3.1250e-05\n",
      "Epoch 66/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9268 - accuracy: 0.7408\n",
      "Epoch 66: val_loss did not improve from 0.98174\n",
      "219/219 [==============================] - 163s 744ms/step - loss: 0.9268 - accuracy: 0.7408 - val_loss: 1.0038 - val_accuracy: 0.7029 - lr: 3.1250e-05\n",
      "Epoch 67/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9283 - accuracy: 0.7318\n",
      "Epoch 67: val_loss did not improve from 0.98174\n",
      "219/219 [==============================] - 165s 756ms/step - loss: 0.9283 - accuracy: 0.7318 - val_loss: 0.9873 - val_accuracy: 0.7079 - lr: 3.1250e-05\n",
      "Epoch 68/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9165 - accuracy: 0.7385\n",
      "Epoch 68: val_loss did not improve from 0.98174\n",
      "219/219 [==============================] - 157s 719ms/step - loss: 0.9165 - accuracy: 0.7385 - val_loss: 1.0033 - val_accuracy: 0.6936 - lr: 3.1250e-05\n",
      "Epoch 69/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9214 - accuracy: 0.7378\n",
      "Epoch 69: val_loss did not improve from 0.98174\n",
      "219/219 [==============================] - 175s 803ms/step - loss: 0.9214 - accuracy: 0.7378 - val_loss: 0.9891 - val_accuracy: 0.7049 - lr: 3.1250e-05\n",
      "Epoch 70/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9229 - accuracy: 0.7378\n",
      "Epoch 70: val_loss improved from 0.98174 to 0.98037, saving model to best_densenet_model.h5\n",
      "219/219 [==============================] - 156s 711ms/step - loss: 0.9229 - accuracy: 0.7378 - val_loss: 0.9804 - val_accuracy: 0.7096 - lr: 3.1250e-05\n",
      "Epoch 71/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9114 - accuracy: 0.7395\n",
      "Epoch 71: val_loss did not improve from 0.98037\n",
      "219/219 [==============================] - 157s 716ms/step - loss: 0.9114 - accuracy: 0.7395 - val_loss: 0.9867 - val_accuracy: 0.7093 - lr: 3.1250e-05\n",
      "Epoch 72/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9172 - accuracy: 0.7423\n",
      "Epoch 72: val_loss did not improve from 0.98037\n",
      "219/219 [==============================] - 154s 704ms/step - loss: 0.9172 - accuracy: 0.7423 - val_loss: 0.9834 - val_accuracy: 0.7056 - lr: 3.1250e-05\n",
      "Epoch 73/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9271 - accuracy: 0.7433\n",
      "Epoch 73: val_loss did not improve from 0.98037\n",
      "219/219 [==============================] - 158s 722ms/step - loss: 0.9271 - accuracy: 0.7433 - val_loss: 0.9919 - val_accuracy: 0.7046 - lr: 3.1250e-05\n",
      "Epoch 74/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9138 - accuracy: 0.7413\n",
      "Epoch 74: val_loss did not improve from 0.98037\n",
      "219/219 [==============================] - 166s 757ms/step - loss: 0.9138 - accuracy: 0.7413 - val_loss: 0.9860 - val_accuracy: 0.7093 - lr: 3.1250e-05\n",
      "Epoch 75/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9182 - accuracy: 0.7448\n",
      "Epoch 75: val_loss improved from 0.98037 to 0.96784, saving model to best_densenet_model.h5\n",
      "219/219 [==============================] - 166s 761ms/step - loss: 0.9182 - accuracy: 0.7448 - val_loss: 0.9678 - val_accuracy: 0.7150 - lr: 3.1250e-05\n",
      "Epoch 76/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9212 - accuracy: 0.7343\n",
      "Epoch 76: val_loss did not improve from 0.96784\n",
      "219/219 [==============================] - 162s 743ms/step - loss: 0.9212 - accuracy: 0.7343 - val_loss: 0.9800 - val_accuracy: 0.7103 - lr: 3.1250e-05\n",
      "Epoch 77/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9134 - accuracy: 0.7385\n",
      "Epoch 77: val_loss did not improve from 0.96784\n",
      "219/219 [==============================] - 164s 750ms/step - loss: 0.9134 - accuracy: 0.7385 - val_loss: 0.9815 - val_accuracy: 0.7106 - lr: 3.1250e-05\n",
      "Epoch 78/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9170 - accuracy: 0.7369\n",
      "Epoch 78: val_loss did not improve from 0.96784\n",
      "219/219 [==============================] - 163s 746ms/step - loss: 0.9170 - accuracy: 0.7369 - val_loss: 0.9802 - val_accuracy: 0.7123 - lr: 3.1250e-05\n",
      "Epoch 79/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9184 - accuracy: 0.7431\n",
      "Epoch 79: val_loss did not improve from 0.96784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 154s 705ms/step - loss: 0.9184 - accuracy: 0.7431 - val_loss: 0.9755 - val_accuracy: 0.7083 - lr: 3.1250e-05\n",
      "Epoch 80/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9105 - accuracy: 0.7409\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.96784\n",
      "219/219 [==============================] - 151s 692ms/step - loss: 0.9105 - accuracy: 0.7409 - val_loss: 0.9718 - val_accuracy: 0.7106 - lr: 3.1250e-05\n",
      "Epoch 81/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9000 - accuracy: 0.7446\n",
      "Epoch 81: val_loss improved from 0.96784 to 0.96730, saving model to best_densenet_model.h5\n",
      "219/219 [==============================] - 154s 705ms/step - loss: 0.9000 - accuracy: 0.7446 - val_loss: 0.9673 - val_accuracy: 0.7130 - lr: 1.5625e-05\n",
      "Epoch 82/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.8969 - accuracy: 0.7382\n",
      "Epoch 82: val_loss improved from 0.96730 to 0.96712, saving model to best_densenet_model.h5\n",
      "219/219 [==============================] - 144s 660ms/step - loss: 0.8969 - accuracy: 0.7382 - val_loss: 0.9671 - val_accuracy: 0.7130 - lr: 1.5625e-05\n",
      "Epoch 83/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9016 - accuracy: 0.7459\n",
      "Epoch 83: val_loss improved from 0.96712 to 0.96426, saving model to best_densenet_model.h5\n",
      "219/219 [==============================] - 141s 647ms/step - loss: 0.9016 - accuracy: 0.7459 - val_loss: 0.9643 - val_accuracy: 0.7123 - lr: 1.5625e-05\n",
      "Epoch 84/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.8834 - accuracy: 0.7536\n",
      "Epoch 84: val_loss did not improve from 0.96426\n",
      "219/219 [==============================] - 139s 637ms/step - loss: 0.8834 - accuracy: 0.7536 - val_loss: 0.9659 - val_accuracy: 0.7116 - lr: 1.5625e-05\n",
      "Epoch 85/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.8980 - accuracy: 0.7464\n",
      "Epoch 85: val_loss improved from 0.96426 to 0.95957, saving model to best_densenet_model.h5\n",
      "219/219 [==============================] - 139s 637ms/step - loss: 0.8980 - accuracy: 0.7464 - val_loss: 0.9596 - val_accuracy: 0.7119 - lr: 1.5625e-05\n",
      "Epoch 86/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.8913 - accuracy: 0.7482\n",
      "Epoch 86: val_loss did not improve from 0.95957\n",
      "219/219 [==============================] - 134s 612ms/step - loss: 0.8913 - accuracy: 0.7482 - val_loss: 0.9663 - val_accuracy: 0.7123 - lr: 1.5625e-05\n",
      "Epoch 87/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.8940 - accuracy: 0.7451\n",
      "Epoch 87: val_loss did not improve from 0.95957\n",
      "219/219 [==============================] - 135s 617ms/step - loss: 0.8940 - accuracy: 0.7451 - val_loss: 0.9645 - val_accuracy: 0.7170 - lr: 1.5625e-05\n",
      "Epoch 88/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.8928 - accuracy: 0.7542\n",
      "Epoch 88: val_loss did not improve from 0.95957\n",
      "219/219 [==============================] - 138s 631ms/step - loss: 0.8928 - accuracy: 0.7542 - val_loss: 0.9628 - val_accuracy: 0.7156 - lr: 1.5625e-05\n",
      "Epoch 89/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.8804 - accuracy: 0.7535\n",
      "Epoch 89: val_loss did not improve from 0.95957\n",
      "219/219 [==============================] - 141s 643ms/step - loss: 0.8804 - accuracy: 0.7535 - val_loss: 0.9612 - val_accuracy: 0.7150 - lr: 1.5625e-05\n",
      "Epoch 90/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.8881 - accuracy: 0.7501\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\n",
      "Epoch 90: val_loss improved from 0.95957 to 0.95951, saving model to best_densenet_model.h5\n",
      "219/219 [==============================] - 139s 637ms/step - loss: 0.8881 - accuracy: 0.7501 - val_loss: 0.9595 - val_accuracy: 0.7150 - lr: 1.5625e-05\n",
      "Epoch 91/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.8836 - accuracy: 0.7545\n",
      "Epoch 91: val_loss improved from 0.95951 to 0.95639, saving model to best_densenet_model.h5\n",
      "219/219 [==============================] - 139s 634ms/step - loss: 0.8836 - accuracy: 0.7545 - val_loss: 0.9564 - val_accuracy: 0.7163 - lr: 7.8125e-06\n",
      "Epoch 92/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.8811 - accuracy: 0.7568\n",
      "Epoch 92: val_loss did not improve from 0.95639\n",
      "219/219 [==============================] - 143s 653ms/step - loss: 0.8811 - accuracy: 0.7568 - val_loss: 0.9614 - val_accuracy: 0.7160 - lr: 7.8125e-06\n",
      "Epoch 93/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.8876 - accuracy: 0.7468\n",
      "Epoch 93: val_loss did not improve from 0.95639\n",
      "219/219 [==============================] - 141s 643ms/step - loss: 0.8876 - accuracy: 0.7468 - val_loss: 0.9586 - val_accuracy: 0.7200 - lr: 7.8125e-06\n",
      "Epoch 94/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.8799 - accuracy: 0.7515\n",
      "Epoch 94: val_loss did not improve from 0.95639\n",
      "219/219 [==============================] - 139s 636ms/step - loss: 0.8799 - accuracy: 0.7515 - val_loss: 0.9584 - val_accuracy: 0.7133 - lr: 7.8125e-06\n",
      "Epoch 95/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.8860 - accuracy: 0.7505\n",
      "Epoch 95: val_loss did not improve from 0.95639\n",
      "219/219 [==============================] - 140s 638ms/step - loss: 0.8860 - accuracy: 0.7505 - val_loss: 0.9595 - val_accuracy: 0.7143 - lr: 7.8125e-06\n",
      "Epoch 96/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.8833 - accuracy: 0.7505\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.95639\n",
      "219/219 [==============================] - 140s 638ms/step - loss: 0.8833 - accuracy: 0.7505 - val_loss: 0.9608 - val_accuracy: 0.7163 - lr: 7.8125e-06\n",
      "Epoch 97/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.8813 - accuracy: 0.7564\n",
      "Epoch 97: val_loss did not improve from 0.95639\n",
      "219/219 [==============================] - 140s 640ms/step - loss: 0.8813 - accuracy: 0.7564 - val_loss: 0.9577 - val_accuracy: 0.7146 - lr: 3.9063e-06\n",
      "Epoch 98/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.8763 - accuracy: 0.7451\n",
      "Epoch 98: val_loss did not improve from 0.95639\n",
      "219/219 [==============================] - 142s 648ms/step - loss: 0.8763 - accuracy: 0.7451 - val_loss: 0.9568 - val_accuracy: 0.7146 - lr: 3.9063e-06\n",
      "Epoch 99/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.8764 - accuracy: 0.7499\n",
      "Epoch 99: val_loss did not improve from 0.95639\n",
      "219/219 [==============================] - 137s 626ms/step - loss: 0.8764 - accuracy: 0.7499 - val_loss: 0.9575 - val_accuracy: 0.7176 - lr: 3.9063e-06\n",
      "Epoch 100/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.8788 - accuracy: 0.7479\n",
      "Epoch 100: val_loss did not improve from 0.95639\n",
      "219/219 [==============================] - 162s 742ms/step - loss: 0.8788 - accuracy: 0.7479 - val_loss: 0.9579 - val_accuracy: 0.7176 - lr: 3.9063e-06\n"
     ]
    }
   ],
   "source": [
    "# Replace 'X_train_processed' and 'y_train' with your training data and labels\n",
    "# Replace 'X_test_processed' and 'y_test' with your validation/test data and labels\n",
    "# Make sure to preprocess your data before using it\n",
    "history = model_densenet.fit(X_train_processed, y_train, \n",
    "                             validation_data=(X_test_processed, y_test), \n",
    "                             batch_size=32, epochs=100, \n",
    "                             callbacks=[lr_scheduler, early_stopping, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e2b64b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54840a75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677dd7b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0cfbdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
